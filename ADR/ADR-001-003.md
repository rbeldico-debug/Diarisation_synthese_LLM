#### ADR-001 : Stratégie de Capture Audio Segmentée par VAD (Silero)
*   **Contexte** : Pour traiter de l'audio en temps réel, on ne peut pas envoyer un flux continu infini à Whisper. Il faut découper l'audio en phrases ou segments logiques. Couper arbitrairement (ex: toutes les 5s) risque de couper un mot en deux.
*   **Décision** : Implémenter une **VAD (Voice Activity Detection)** légère (Silero-VAD) directement sur le flux microphonique brut.
*   **Fonctionnement technique** :
    1.  Le micro écoute en permanence.
    2.  Silero-VAD (très rapide, < 20ms de latence) analyse si c'est "de la parole" ou "du silence".
    3.  Tant que ça parle, on remplit un `buffer`.
    4.  Dès qu'un silence de plus de X ms (ex: 500ms) est détecté, on considère la phrase finie.
    5.  Le `buffer` est clôturé et envoyé dans une `Queue` pour traitement.
*   **Justification** : Silero est standard, gratuit et pré-entraîné. Cette méthode garantit qu'on envoie à Whisper des phrases complètes, améliorant drastiquement la qualité de la transcription.
*   **Conséquences** : Ajoute une légère dépendance logicielle (`silero-vad` ou via `torch`), mais essentielle pour la qualité.

#### ADR-002 : Moteur de Transcription et Diarisation (Pipeline Unifié)
*   **Contexte** : Nous avons besoin de savoir "qui a dit quoi". La documentation de Faster-Whisper et Pyannote montre que ces outils fonctionnent par "fichiers" ou "segments". Avec 16GB de VRAM, nous avons de la marge.
*   **Décision** : Utiliser un pipeline séquentiel par segment : **Pyannote (Diarisation)** -> **Faster-Whisper (Transcription)**.
    *   *Pourquoi cet ordre ?* Pyannote est très fort pour dire "Speaker A parle de 0.0s à 4.5s". Ensuite, on peut guider Whisper ou recaler le texte sur ces segments. (Ou l'inverse : Whisper transcrit tout, et on aligne les timestamps ensuite. L'approche "Whisper d'abord" est souvent plus simple à implémenter au début).
    *   **Configuration retenue** :
        *   **Whisper** : Modèle `large-v3` avec `compute_type="float16"` (ou `int8` si besoin de place pour le LLM). Utilisation de CTranslate2 (Faster-Whisper).
        *   **Pyannote** : Pipeline `speaker-diarization-community-1` (version 3.1).
*   **Justification** : `Faster-whisper` est validé par tes docs comme étant 4x plus rapide. La VRAM de 16GB permet d'éviter les modèles "tiny" ou "base" qui font beaucoup d'erreurs.
*   **Conséquences** : Le traitement d'un segment audio de 5 secondes prendra environ 0.5 à 1 seconde. C'est du "quasi temps réel".

#### ADR-003 : Architecture de Données "Producer-Consumer"
*   **Contexte** : La capture audio ne doit JAMAIS bloquer, même si Whisper met 1 seconde à transcrire.
*   **Décision** : Utiliser le module `multiprocessing` de Python avec deux processus distincts communiquant via une `Queue`.
    *   **Process 1 (L'Oreille)** : Capture audio + VAD + Découpage en segments -> `Queue`.
    *   **Process 2 (Le Cerveau)** : Lit la `Queue` -> Whisper -> Pyannote -> Mise en forme -> Sortie.
*   **Justification** : Sépare les responsabilités. Si le Cerveau sature, l'Oreille continue d'enregistrer et empile les segments dans la file d'attente (bufferisation) au lieu de perdre des données.
*   **Conséquences** : Architecture un peu plus complexe à déboguer qu'un script linéaire simple, mais indispensable pour la fluidité.
