import time
import queue
import logging
from datetime import datetime
from pathlib import Path

from core.settings import settings
from core.data_models import AudioPayload
from brain.sanitizer import TextSanitizer

# Modules M√©tier
from brain.inference_client import InferenceClient
from brain.router import IntentRouter
from brain.graph.manager import GraphStateManager
from analyst.synthesizer import Synthesizer
from memory.storage_manager import MemoryManager
from memory.vector_manager import VectorManager
from memory.librarian import Librarian


class BrainOrchestrator:
    # AJOUT de input_queue dans les arguments
    def __init__(self, audio_queue: queue.Queue, tts_queue: queue.Queue, input_queue: queue.Queue, stop_event):
        self.audio_queue = audio_queue
        self.tts_queue = tts_queue
        self.input_queue = input_queue  # <--- Nouveau
        self.stop_event = stop_event

        print("[Orchestrator] üß† Initialisation du Cortex...")
        # ... (Reste de l'init inchang√© : chargement moteurs, etc.) ...
        self.inference = InferenceClient()
        self.router = IntentRouter()
        self.graph = GraphStateManager()
        self.memory = MemoryManager()
        self.vectors = VectorManager()
        self.librarian = Librarian()
        self.synthesizer = Synthesizer(graph_manager=self.graph)

        self.graph.load_state()
        self.inference.warm_up()
        self.graph.export_activity_snapshot(settings.LOGS_DIR / "brain_activity.json")

        self.last_propagation = time.time()
        self.last_decay = time.time()
        self.last_gardening = time.time()

        print("[Orchestrator] ‚úÖ Syst√®me Pr√™t.")

    def run(self):
        """Boucle Principale"""
        while not self.stop_event.is_set():
            try:
                # 1. Check Texte (Priorit√© max)
                try:
                    msg_type, content = self.input_queue.get_nowait()
                    if msg_type == "text":
                        self.process_text_input(content)
                except queue.Empty:
                    pass

                # 2. Check Audio (Priorit√© haute)
                # On utilise get_nowait ou timeout tr√®s court pour ne pas bloquer le texte
                try:
                    audio_payload = self.audio_queue.get(timeout=0.1)
                    self.process_interaction(audio_payload)
                except queue.Empty:
                    # 3. T√¢ches de fond (Si rien d'autre)
                    self.process_background_tasks()

            except Exception as e:
                print(f"[Orchestrator] Erreur Loop: {e}")

    def process_text_input(self, text: str):
        """Entr√©e Texte (Clavier)"""
        print(f"\n[Flux Texte] ‚å®Ô∏è {text}")
        # On d√©l√®gue √† la logique centrale
        self._execute_intent(text, source="Clavier")

    def process_interaction(self, payload: AudioPayload):
        """Entr√©e Audio (Microphone)"""
        # 1. Transcription (Whisper)
        text, speakers = self.inference.process_audio(payload.audio_data, payload.sample_rate)

        if not TextSanitizer.is_valid(text):
            return

        print(f"\n[Flux Audio] üó£Ô∏è {text}")

        # On d√©l√®gue √† la logique centrale
        self._execute_intent(text, source="Vocal")

        # --- LOGIQUE CENTRALE (Cerveau) ---

    def _execute_intent(self, text: str, source: str):
        """
        C≈ìur d√©cisionnel : Route -> Agit.
        """
        # 1. Identification de l'intention (Mistral Nemo)
        intent = self.router.route(text)
        print(f"[Orchestrator] Intention : {intent}")

        # 2. Aiguillage
        if intent == "[READ]":
            # Mode Assistant : On r√©pond √† l'utilisateur
            self._handle_read_intent(text, source)

        elif intent == "[WRITE]":
            # Mode Prise de Note : On enregistre et on se tait
            self._handle_write_intent(text, source, intent_tag=intent)

        elif intent == "[CHAT]":
            # Mode Conversation : On enregistre comme du Write pour l'instant
            # (Plus tard on pourra ajouter une r√©ponse "Chat" pure sans note)
            self._handle_write_intent(text, source, intent_tag=intent)

        elif intent == "[CMD]":
            print("[Orchestrator] Commande re√ßue (Non impl√©ment√©).")

        # --- HANDLERS SP√âCIFIQUES ---

    def _handle_write_intent(self, text: str, source: str, intent_tag: str):
        """
        Pipeline classique : Stimulus -> Vector -> Dashboard -> Librarian (Inbox)
        """
        # 1. Injection Stimulus (R√©veil Graphe)
        self.graph.inject_stimulus(text, intent_tag)

        # 2. Log Journal (M√©moire Court Terme)
        self.memory.log_event(source=source, text=text, intent=intent_tag)

        # 3. M√©moire Vectorielle (Long Terme)
        embedding = self.router.get_embedding(text)
        if embedding is not None:
            self.vectors.add_to_memory(text, embedding, {
                "timestamp": datetime.now().isoformat(),
                "session": "current"
            })

        # 4. Synth√®se Dashboard (Mise √† jour Web)
        dashboard_md, concepts = self.synthesizer.generate_summary()
        self.memory.update_dashboard(dashboard_md)

        # 5. Extraction de Concepts (Vers 00_Inbox)
        if concepts:
            print(f"[Orchestrator] üí° {len(concepts)} concepts extraits -> Inbox.")
            for concept in concepts:
                self.librarian.process_concept(concept['title'], concept['content'], concept['tags'])

    def _handle_read_intent(self, text: str, source: str):
        """
        Pipeline RAG + TTS : Recherche -> Synth√®se -> Parole
        """
        print("[Orchestrator] üîç Recherche d'information...")

        # 1. Log de la demande
        self.memory.log_event(source=source, text=text, intent="[READ]")

        # 2. Recherche RAG (Vecteurs + Graphe)
        context = []

        # A. Vecteurs (Ce qu'on a d√©j√† dit)
        emb = self.router.get_embedding(text)
        if emb is not None:
            res = self.vectors.search_similar(emb, n_results=3)
            if res and res['documents']:
                context.extend(res['documents'][0])

        # B. Graphe (Ce qui est activ√©/Reli√©)
        # On pourrait chercher les n≈ìuds dont le titre ressemble √† la demande
        # Pour l'instant on prend les n≈ìuds actifs
        active_nodes = sorted([n for n in self.graph.nodes.values() if n.activation > 0],
                              key=lambda x: x.activation, reverse=True)[:3]
        for n in active_nodes:
            context.append(f"Concept pertinent : {n.title}")

        # 3. G√©n√©ration de la r√©ponse vocale (LLM)
        context_str = "\n".join(context)
        system_prompt = (
            "Tu es Oc√©ane. L'utilisateur te pose une question sur sa base de connaissance.\n"
            "R√©ponds bri√®vement (max 2 phrases) en utilisant le CONTEXTE fourni.\n"
            "Si tu ne sais pas, dis-le simplement."
        )

        try:
            from openai import OpenAI  # Import local pour √©viter conflit si non charg√© globalement
            client = OpenAI(base_url=settings.LLM_BASE_URL, api_key="ollama")

            response = client.chat.completions.create(
                model=settings.LLM_MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"CONTEXTE:\n{context_str}\n\nQUESTION: {text}"}
                ],
                temperature=0.7
            )
            answer = response.choices[0].message.content

            # 4. Affichage & Parole
            print(f"[Oc√©ane] üó£Ô∏è {answer}")
            self.memory.log_event(source="Oc√©ane", text=answer, intent="[REPONSE]")

            # Envoi √† la Bouche (TTS)
            self.tts_queue.put(answer)

        except Exception as e:
            print(f"[Orchestrator] Erreur Read Intent: {e}")

    def process_background_tasks(self):
        """Maintenance du syst√®me quand l'utilisateur ne parle pas."""
        now = time.time()

        # A. Propagation de l'Activation (Toutes les 2s)
        if now - self.last_propagation > 2.0:
            self.graph.propagate_activation()
            # Export JSON pour le Web
            self.graph.export_activity_snapshot(settings.LOGS_DIR / "brain_activity.json")
            self.last_propagation = now

        # B. Oubli & Fatigue (Toutes les 10s pour √™tre plus r√©actif)
        if now - self.last_decay > 10.0:
            for node in self.graph.nodes.values():
                node.decay()
                node.rest()
            self.last_decay = now

        # C. Jardinage Automatique (Toutes les 60s)
        # C'est ici qu'on applique vos r√®gles (Graine -> Sapling)
        if now - self.last_gardening > 60.0:
            self._gardening_cycle()
            self.last_gardening = now

    def _gardening_cycle(self):
        """
        Applique les r√®gles algorithmiques de '00_tags.md'.
        """
        # On ne scanne pas tout pour ne pas geler le PC, juste un √©chantillon ou les actifs
        # Pour l'instant, on fait un pass sur les n≈ìuds en m√©moire RAM
        changes_count = 0

        for node in self.graph.nodes.values():
            # R√àGLE 1 : Graine -> Sapling
            # SI #√©tat/graine ET (liens > 2) -> #√©tat/sapling
            if "√©tat/graine" in node.tags and len(node.links) > 2:
                print(f"[Jardinier] üå± -> üå≥ Croissance d√©tect√©e : {node.title}")
                node.tags.remove("√©tat/graine")
                node.tags.add("√©tat/sapling")
                # TODO: R√©percuter la modif dans le fichier Markdown physique via Librarian
                changes_count += 1

            # R√àGLE 2 : Archivage (Apoptose)
            # SI non modifi√© depuis 2 ans (730 jours) -> #archives
            days_inactive = (datetime.now() - node.date_updated).days
            if days_inactive > 730 and "archives" not in str(node.tags):
                print(f"[Jardinier] üçÇ Archivage auto : {node.title}")
                # Logic d'archivage √† impl√©menter
                pass

        if changes_count > 0:
            print(f"[Jardinier] {changes_count} mises √† jour effectu√©es.")
            self.graph.save_state()