# üß† Local AI Voice Assistant (LAVA) - v2.0 (Docker Edition)

Assistant vocal local modulaire utilisant une architecture **Micro-services**. 
L'IA lourde est isol√©e dans Docker pour garantir une stabilit√© totale des d√©pendances.

## üìä Architecture du Flux

```mermaid
graph TD
    subgraph "H√îTE WINDOWS (Python 3.11)"
        A[Microphone] --> B(VAD Silero)
        B -->|Segment Audio| C[Main Process]
    end

    subgraph "DOCKER (Isol√© - GPU)"
        C -->|HTTP/API| D[Speaches Server]
        D -->|Whisper Large-v3| E(Transcription)
        D -->|Pyannote 3.1| F(Diarization)
    end

    subgraph "OLLAMA (Service Externe)"
        C -->|HTTP/API| G[LLM - gpt-oss:20b]
    end

    E & F -->|R√©sultats JSON| C
    G -->|R√©ponse vocale| H[Sortie Console/Audio]
```

## üõ†Ô∏è Pr√©-requis

*   **OS** : Windows 10/11 avec **Docker Desktop** (WSL2 backend).
*   **GPU** : NVIDIA RTX (drivers √† jour).
*   **Outils** : Ollama (install√© sur Windows).

## üöÄ Installation Rapide

1.  **Pr√©parer l'environnement Python** :
    ```bash
    python -m venv .venv
    .venv\Scripts\activate
    pip install -r requirements.txt
    ```

2.  **Lancer le moteur IA (Docker)** :
    *   Remplissez votre `HF_TOKEN` dans le fichier `docker-compose.yml`.
    *   Dans un terminal, lancez :
    ```bash
    docker compose up -d
    ```

3.  **V√©rifier le serveur** :
    Acc√©dez √† [http://localhost:8000/docs](http://localhost:8000/docs). Si la page s'affiche, le moteur est pr√™t.

## ‚öôÔ∏è Configuration

*   **Diarisation** : Automatiquement g√©r√©e par le serveur Speaches via l'argument `extra_body={"diarization": True}` dans `diarization.py`.
*   **VRAM** : Le mod√®le `large-v3` consomme environ 5-6 Go de VRAM sur votre GPU via Docker.

## ‚ñ∂Ô∏è Utilisation

1.  Lancer Ollama.
2.  Lancer le conteneur Docker (si pas d√©j√† fait).
3.  Ex√©cuter le programme :
    ```bash
    python main.py
    ```

## üíé Avantages de la v2.0
*   **Z√©ro Conflit** : Plus de probl√®me de version NumPy ou de DLL CUDA manquantes.
*   **Performance** : Le mod√®le Whisper est pr√©-charg√© dans la VRAM par Docker, √©liminant la latence de chargement au premier mot.
*   **Portabilit√©** : Le code Python est devenu un simple client API ultra-l√©ger.


---

### 4. Rappel du fichier `docker-compose.yml` (√Ä placer √† la racine)
C'est le fichier qui "sauve" ton projet.

```yaml
services:
  whisper:
    image: ghcr.io/speaches-ai/speaches:latest-cuda
    container_name: whisper-server
    ports:
      - "8000:8000"
    volumes:
      - hf-hub-cache:/root/.cache/huggingface/hub
    environment:
      - HF_TOKEN=hf_ton_token_ici
      - WHISPER__MODEL=Systran/faster-whisper-large-v3
      - WHISPER__DEVICE=cuda
      - WHISPER__COMPUTE_TYPE=float16
      - WHISPER__TTL=-1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  hf-hub-cache:
```
