# üåä Oc√©ane - Assistant de R√©flexion S√©mantique (v2.5)

Oc√©ane est un √©cosyst√®me multi-agents local con√ßu pour transformer vos r√©flexions orales en un **Zettelkasten** structur√© dans Obsidian.

## üèóÔ∏è Architecture des Agents
1. **P1: L'Oreille** (Silero VAD) : D√©coupe le flux audio en segments logiques.
2. **P2: Le Cerveau** (Whisper + Semantic Router) : Transcrit, identifie les locuteurs et classe les intentions via **Similarit√© Cosinus** (pas de LLM pour le tri, 100% fiable).
3. **P3: La Bouche** (Edge-TTS) : Restitue des briefings vocaux haute fid√©lit√© (Voix: Vivienne).
4. **P4: L'Analyste** (Mistral-Nemo + RAG) : Analyse le journal JSONL, interroge **ChromaDB** pour retrouver des souvenirs pass√©s, et sculpte le Dashboard Obsidian.

## üß† Fonctions Avanc√©es
- **Similarit√© S√©mantique** : Plus d'hallucination de tags. Le syst√®me compare math√©matiquement vos propos √† la Taxonomie Universelle.
- **RAG (Retrieval Augmented Generation)** : L'analyste cr√©e des liens `[[WikiLinks]]` entre vos propos actuels et vos r√©flexions des sessions pr√©c√©dentes.
- **Zettelkasten Automatique** : √Ä chaque arr√™t, une note atomique format√©e est archiv√©e dans votre Vault Obsidian.

## üõ†Ô∏è Maintenance & Commandes
- **D√©marrage** : `execute.bat` (Lance Docker Whisper, ChromaDB, Ollama et le script Main).
- **Arr√™t propre** : `python stop.py` (D√©clenche l'archivage final et coupe les processus).

## üìä Architecture du Flux

```mermaid
graph TD
    subgraph "H√îTE WINDOWS (Python 3.11)"
        A[Microphone] --> B(P1: Oreille - Silero VAD)
        B -->|Segment Audio| C(P2: Cerveau - Orchestrateur)
        C -->|Texte + Intent| D[(Journal.jsonl - Journaling)]
        D --> E(P4: Analyste - Synth√®se)
        E -->|Markdown| F[Dashboard.md - Zettelkasten]
        C -->|Texte R√©ponse| G(P3: Bouche - Piper TTS)
    end

    subgraph "DOCKER (Isol√© - GPU)"
        C -->|API| H[Speaches: Whisper + Diarization]
        C -->|API| I[Router: Llama 1B]
    end

    subgraph "OLLAMA (Service Externe)"
        C -->|API| J[LLM Principal: gpt-oss-20b]
        E -->|API| J
    end
```

## üõ†Ô∏è Pr√©-requis

*   **OS** : Windows 10/11 avec **Docker Desktop** (WSL2 backend).
*   **GPU** : NVIDIA RTX (drivers √† jour).
*   **Outils** : Ollama (install√© sur Windows).

## üöÄ Installation Rapide

1.  **Pr√©parer l'environnement Python** :
    ```bash
    python -m venv .venv
    .venv\Scripts\activate
    pip install -r requirements.txt
    ```

2.  **Lancer le moteur IA (Docker)** :
    *   Remplissez votre `HF_TOKEN` dans le fichier `docker-compose.yml`.
    *   Dans un terminal, lancez :
    ```bash
    docker compose up -d
    ```

3.  **V√©rifier le serveur** :
    Acc√©dez √† [http://localhost:8000/docs](http://localhost:8000/docs). Si la page s'affiche, le moteur est pr√™t.

## ‚öôÔ∏è Configuration

*   **Diarisation** : Automatiquement g√©r√©e par le serveur Speaches via l'argument `extra_body={"diarization": True}` dans `diarization.py`.
*   **VRAM** : Le mod√®le `large-v3` consomme environ 5-6 Go de VRAM sur votre GPU via Docker.

## ‚ñ∂Ô∏è Utilisation

1.  Lancer Ollama.
2.  Lancer le conteneur Docker (si pas d√©j√† fait).
3.  Ex√©cuter le programme :
    ```bash
    python main.py
    ```

## üíé Avantages de la v2.0
*   **Z√©ro Conflit** : Plus de probl√®me de version NumPy ou de DLL CUDA manquantes.
*   **Performance** : Le mod√®le Whisper est pr√©-charg√© dans la VRAM par Docker, √©liminant la latence de chargement au premier mot.
*   **Portabilit√©** : Le code Python est devenu un simple client API ultra-l√©ger.


---

### 4. Rappel du fichier `docker-compose.yml` (√Ä placer √† la racine)
C'est le fichier qui "sauve" ton projet.

```yaml
name: Diarisation_Synthese_LLM

services:
  # --- MOTEUR AUDIO (Whisper) ---
  whisper:
    image: ghcr.io/speaches-ai/speaches:latest-cuda
    container_name: whisper-server
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - whisper_data:/home/ubuntu/.cache/huggingface
    environment:
      - HF_TOKEN=[CLE API ICI]
      - HF_HOME=/home/ubuntu/.cache/huggingface
      - SPEACHES_MODELS_PRELOAD=Systran/faster-whisper-large-v3
      - WHISPER__MODEL=Systran/faster-whisper-large-v3
      - WHISPER__DEVICE=cuda
      - WHISPER__COMPUTE_TYPE=int8_float16
      - WHISPER__NUM_WORKERS=4
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # --- LE CERVEAU UNIQUE (Ollama: Nemo + Embeddings) ---
  router-llm:
    image: ollama/ollama:latest
    container_name: LLM-router
    restart: unless-stopped
    ports:
      - "11435:11434"
    volumes:
      - ollama_storage:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # --- LA M√âMOIRE VECTORIELLE (ChromaDB) ---
  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    container_name: chromadb-server
    restart: unless-stopped
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma # Volume persistant pour le RAG
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE

volumes:
  whisper_data:
  ollama_storage:
  chroma_data:
```
