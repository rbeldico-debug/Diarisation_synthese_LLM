# üß† Local AI Voice Assistant (LAVA) - v2.0 (Docker Edition)

Assistant vocal local modulaire utilisant une architecture **Micro-services**. 
L'IA lourde est isol√©e dans Docker pour garantir une stabilit√© totale des d√©pendances.

## üìä Architecture du Flux

```mermaid
graph TD
    subgraph "H√îTE WINDOWS (Python 3.11)"
        A[Microphone] --> B(P1: Oreille - Silero VAD)
        B -->|Segment Audio| C(P2: Cerveau - Orchestrateur)
        C -->|Texte + Intent| D[(Journal.jsonl - Journaling)]
        D --> E(P4: Analyste - Synth√®se)
        E -->|Markdown| F[Dashboard.md - Zettelkasten]
        C -->|Texte R√©ponse| G(P3: Bouche - Piper TTS)
    end

    subgraph "DOCKER (Isol√© - GPU)"
        C -->|API| H[Speaches: Whisper + Diarization]
        C -->|API| I[Router: Llama 1B]
    end

    subgraph "OLLAMA (Service Externe)"
        C -->|API| J[LLM Principal: gpt-oss-20b]
        E -->|API| J
    end
```

## üõ†Ô∏è Pr√©-requis

*   **OS** : Windows 10/11 avec **Docker Desktop** (WSL2 backend).
*   **GPU** : NVIDIA RTX (drivers √† jour).
*   **Outils** : Ollama (install√© sur Windows).

## üöÄ Installation Rapide

1.  **Pr√©parer l'environnement Python** :
    ```bash
    python -m venv .venv
    .venv\Scripts\activate
    pip install -r requirements.txt
    ```

2.  **Lancer le moteur IA (Docker)** :
    *   Remplissez votre `HF_TOKEN` dans le fichier `docker-compose.yml`.
    *   Dans un terminal, lancez :
    ```bash
    docker compose up -d
    ```

3.  **V√©rifier le serveur** :
    Acc√©dez √† [http://localhost:8000/docs](http://localhost:8000/docs). Si la page s'affiche, le moteur est pr√™t.

## ‚öôÔ∏è Configuration

*   **Diarisation** : Automatiquement g√©r√©e par le serveur Speaches via l'argument `extra_body={"diarization": True}` dans `diarization.py`.
*   **VRAM** : Le mod√®le `large-v3` consomme environ 5-6 Go de VRAM sur votre GPU via Docker.

## ‚ñ∂Ô∏è Utilisation

1.  Lancer Ollama.
2.  Lancer le conteneur Docker (si pas d√©j√† fait).
3.  Ex√©cuter le programme :
    ```bash
    python main.py
    ```

## üíé Avantages de la v2.0
*   **Z√©ro Conflit** : Plus de probl√®me de version NumPy ou de DLL CUDA manquantes.
*   **Performance** : Le mod√®le Whisper est pr√©-charg√© dans la VRAM par Docker, √©liminant la latence de chargement au premier mot.
*   **Portabilit√©** : Le code Python est devenu un simple client API ultra-l√©ger.


---

### 4. Rappel du fichier `docker-compose.yml` (√Ä placer √† la racine)
C'est le fichier qui "sauve" ton projet.

```yaml
name: Diarisation_Synthese_LLM

services:
  whisper:
    image: ghcr.io/speaches-ai/speaches:latest-cuda
    container_name: whisper-server
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - whisper_data:/home/ubuntu/.cache/huggingface
    environment:
      - HF_TOKEN=[VOTRE CLE API]
      - HF_HOME=/home/ubuntu/.cache/huggingface
      - SPEACHES_MODELS_PRELOAD=Systran/faster-whisper-large-v3
      - WHISPER__MODEL=Systran/faster-whisper-large-v3
      - WHISPER__DEVICE=cuda
      - WHISPER__COMPUTE_TYPE=int8_float16
      - WHISPER__NUM_WORKERS=4
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  router-llm:
    image: ollama/ollama:latest
    container_name: LLM-router
    restart: unless-stopped
    ports:
      - "11435:11434"
    volumes:
      - ollama_storage:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  whisper_data:
  ollama_storage:
```
